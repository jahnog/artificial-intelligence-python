{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0cf8cd7b9d9729a69e355ba433e658c2b1bd402d6067d224ac5b6cecdc2049b2d",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "cf8cd7b9d9729a69e355ba433e658c2b1bd402d6067d224ac5b6cecdc2049b2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource():\n",
    "    def get_training_data(self):\n",
    "\n",
    "        column_names = ['MPG', 'Cylinders', 'Displacement',\n",
    "                        'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin']\n",
    "        raw_dataset = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\",\n",
    "                                  names=column_names, na_values=\"?\", comment='\\t', sep=\" \", skipinitialspace=True)\n",
    "\n",
    "        dataset = raw_dataset.copy()\n",
    "        dataset = dataset.dropna()\n",
    "\n",
    "        origin = dataset.pop('Origin')\n",
    "        dataset['USA'] = (origin == 1)*1.0\n",
    "        dataset['Europe'] = (origin == 2)*1.0\n",
    "        dataset['Japan'] = (origin == 3)*1.0\n",
    "\n",
    "        desc = dataset.describe()\n",
    "\n",
    "        train_stats = dataset.describe()\n",
    "        train_stats.pop(\"MPG\")\n",
    "        train_stats = train_stats.transpose()\n",
    "\n",
    "        labels = dataset.pop('MPG')\n",
    "\n",
    "        def norm(x):\n",
    "            return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "        normed_data = norm(dataset)\n",
    "\n",
    "        X_raw = normed_data.to_numpy().T\n",
    "\n",
    "        Y_raw = np.reshape(labels.to_numpy(), (1, len(labels)))\n",
    "\n",
    "        X_train = X_raw[:, :-10]\n",
    "        Y_train = Y_raw[:, :-10]\n",
    "        X_test = X_raw[:, -10:]\n",
    "        Y_test = Y_raw[:, -10:]\n",
    "\n",
    "        return X_train, Y_train, X_test, Y_test, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FourLayerModel:\n",
    "    def __init__(self, input_features: int, hidden_layer1_neurons: int = 10, hidden_layer2_neurons: int = 5, output_layer_neurons: int = 1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            input_features (int): The number of input features.\n",
    "            hidden_layer1_neurons (int, optional): Number of Neurons for the first hidden layer. Defaults to 10.\n",
    "            hidden_layer2_neurons (int, optional): Number of Neurons for the second hidden layer. Defaults to 5.\n",
    "            output_layer_neurons (int, optional): Number of Neurons for the output layer. Defaults to 1.\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.W1 = np.random.randn(hidden_layer1_neurons, input_features) * 0.01\n",
    "        self.b1 = np.zeros((hidden_layer1_neurons, 1))\n",
    "\n",
    "        self.W2 = np.random.randn(hidden_layer2_neurons, hidden_layer1_neurons) * 0.01\n",
    "        self.b2 = np.zeros((hidden_layer2_neurons, 1))\n",
    "\n",
    "        self.W3 = np.random.randn(output_layer_neurons, hidden_layer2_neurons) * 0.01\n",
    "        self.b3 = np.zeros((output_layer_neurons, 1))\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(Z, 0)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "        X -- Input data of shape (number of features, number of examples).\n",
    "\n",
    "        Returns:\n",
    "        The prediction of the model, of shape (1, number of examples)\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate first hidden layer neurons\n",
    "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
    "        # Calculate first hidden layer neuron activations\n",
    "        self.A1 = self.relu(self.Z1)\n",
    "\n",
    "        # Calculate second hidden layer neurons\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
    "        # Calculate second hidden layer neuron activations\n",
    "        self.A2 = self.relu(self.Z2)\n",
    "\n",
    "        # Calculate output layer neurons\n",
    "        self.Z3 = np.dot(self.W3, self.A2) + self.b3\n",
    "        # Calculate output layer neuron activations\n",
    "        self.A3 = self.Z3\n",
    "\n",
    "        return self.A3\n",
    "\n",
    "    def calculate_cost(self, prediction, Y):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        prediction -- The output of last activation, of shape (1, number of examples)\n",
    "        Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "        Returns:\n",
    "        cost -- computes the Mean Squared Error\n",
    "        \"\"\"\n",
    "\n",
    "        return np.mean(np.square(Y - prediction))\n",
    "\n",
    "    def relu_dev(self, X):\n",
    "        return np.where(X > 0, 1, 0)\n",
    "\n",
    "    def backward_propagation(self, X, Y):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        X -- Input data of shape (number of features, number of examples).\n",
    "        Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "        \"\"\"\n",
    "\n",
    "        # Number of examples\n",
    "        m = X.shape[1]\n",
    "\n",
    "        # Mean Suared Error derivative\n",
    "        self.dZ3 = -2 * (Y - self.A3)\n",
    "\n",
    "        # Output layer derivatives\n",
    "        self.dW3 = np.dot(self.dZ3, self.A2.T) / m\n",
    "        self.db3 = np.sum(self.dZ3, axis=1, keepdims=True) / m\n",
    "\n",
    "        # Second hidden layer derivatives\n",
    "        self.dZ2 = np.dot(self.W3.T, self.dZ3) * self.relu_dev(self.A2)\n",
    "        self.dW2 = np.dot(self.dZ2, self.A1.T) / m\n",
    "        self.db2 = np.sum(self.dZ2, axis=1, keepdims=True) / m\n",
    "\n",
    "        # First hidden layer derivatives\n",
    "        self.dZ1 = np.dot(self.W2.T, self.dZ2) * self.relu_dev(self.A1)\n",
    "        self.dW1 = np.dot(self.dZ1, X.T) / m\n",
    "        self.db1 = np.sum(self.dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "    def update_weights(self, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "        learning_rate -- The learning rate to apply in the weights update.\n",
    "        \"\"\"\n",
    "        self.W1 = self.W1 - learning_rate * self.dW1\n",
    "        self.b1 = self.b1 - learning_rate * self.db1\n",
    "        self.W2 = self.W2 - learning_rate * self.dW2\n",
    "        self.b2 = self.b2 - learning_rate * self.db2\n",
    "        self.W3 = self.W3 - learning_rate * self.dW3\n",
    "        self.b3 = self.b3 - learning_rate * self.db3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, desc = DataSource().get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EPOCHS = 15001\n",
    "LEARNING_RATE = 0.001\n",
    "PRINT_EACH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = FourLayerModel(X_train.shape[0], 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cost after iteration 0: 601.210960 \n",
      "Cost after iteration 1000: 7.917978 \n",
      "Cost after iteration 2000: 7.495592 \n",
      "Cost after iteration 3000: 6.847989 \n",
      "Cost after iteration 4000: 6.512429 \n",
      "Cost after iteration 5000: 6.316242 \n",
      "Cost after iteration 6000: 6.199060 \n",
      "Cost after iteration 7000: 6.104805 \n",
      "Cost after iteration 8000: 6.063787 \n",
      "Cost after iteration 9000: 6.036635 \n",
      "Cost after iteration 10000: 6.019376 \n",
      "Cost after iteration 11000: 6.007958 \n",
      "Cost after iteration 12000: 5.997156 \n",
      "Cost after iteration 13000: 5.990236 \n",
      "Cost after iteration 14000: 5.984817 \n",
      "Cost after iteration 15000: 5.979764 \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, TRAIN_EPOCHS):\n",
    "    predictions = nn.forward_propagation(X_train)\n",
    "    cost = nn.calculate_cost(predictions, Y_train)\n",
    "\n",
    "    nn.backward_propagation(X_train, Y_train)\n",
    "\n",
    "    nn.update_weights(LEARNING_RATE)\n",
    "\n",
    "    if (i) % PRINT_EACH == 0:\n",
    "        print(\"Cost after iteration %i: %f \" % (i, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predictions:  [[29 26 30 32 25 28 42 34 29 28]]\nReal Values:  [[26 22 32 36 27 27 44 32 28 31]]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = nn.forward_propagation(X_test)\n",
    "\n",
    "print(\"Predictions: \", test_predictions.astype(int))\n",
    "print(\"Real Values: \", Y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}